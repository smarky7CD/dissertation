\chapter{Introduction}

\authorRemark{The following needs to be re-organized and re-written.}

Data structures define representations of possibly dynamic (multi)sets, along with the operations that can be performed on this representation of the underlying data. Efficient data structures are crucial for designing efficient algorithms~\cite{clrs}. The development and analysis of data structures has largely been driven by operational concerns, e.g., efficiency, ease of deployment, support for broad application. Security concerns, on the other hand, have traditionally been afterthoughts (at best). However, recent research has highlighted that many widely-used data structures do not behave as expected when in the presence of adversaries that have the ability to control the data they represent. Further, complex protocols that have sophisticated security goals are increasingly using a variety of bespoke data structures as fundamental components of their design. Therefore, it is wise to begin applying the provable security paradigm to data structures themselves. 

For instance, consider probabilistic data structures (PDS). They provide compact (sublinear) representations of potentially large collections of data and support a small set of queries that can be answered efficiently. Prime examples of such structures include the Bloom filter~\cite{bloom1970space}, the HyperLogLog~\cite{flajolet2007hyperloglog}, and the Count-min Sketch~\cite{cormode2005improved}. These space and (by extension) performance gains come at the expense of correctness.  Specifically, PDS query responses are computed over the compact representation of the data, as opposed to the complete data.  As a result, PDS query responses are only guaranteed to be \emph{close} to the true answer with \emph{large} probability, where \emph{close} and \emph{large} are typically functions of structure parameters (e.g., the representation size) and properties of the data. These guarantees are stated under the assumption that the data and the internal randomness of the PDS are independent.  Informally, this is tantamount to assuming  that the entire collection of data is (or can be) determined \emph{before} any random choices are made by the PDS.  For many PDS, this means before some number of hash functions are sampled, as the PDS operates deterministically after that. Recent works have begun to explore the impact on correctness guarantees for data that \emph{may} depend upon the internal randomness of the structure, and the initial findings are negative. 

Moreover, consider the class of data structures we refer to as \emph{skipping data structures}. Unlike the probabilistic data structures we discussed earlier, this class of structure are not space-efficient (compact) and, in turn, give exact answers to queries. These data structures (e.g., hash tables, skip lists, and treaps) offer fast average-case runtime of their operations, but have worst-case runtime that is  poor. They achieve this by using some form of randomness to determine the representation of the underlying data collection.  Recent research shows that adaptive adversaries are able to force worst-case runtime for these structure, often demonstrated by attacks on real-world systems. Therefore, instead of focusing on adversarial correctness as in the PDS section, we focus on preserving the expected run time of these structures with large probability in the presence of an adversary.