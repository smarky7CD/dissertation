\chapter{Introduction}

\authorRemark{The following needs to be re-organized and re-written.}

% CFE Work

Count-Min Sketch (CMS) and HeavyKeeper (HK) are two realizations of a compact frequency estimator (CFE).  These are a class of probabilistic data structures that maintain a compact summary of (typically) high-volume streaming data, and provides approximately correct estimates of the number of times any particular element has appeared. CFEs are often the base structure in systems looking for the highest-frequency elements (i.e., top-$K$ elements, heavy hitters, elephant flows).  Traditionally, probabilistic guarantees on the accuracy of frequency estimates are proved under the implicit assumption that stream elements do not depend upon the internal randomness of the structure. Said another way, they are proved in the presence of data streams that are created by non-adaptive adversaries.  Yet in many practical use-cases, this assumption is not well-matched with reality; especially, in applications where malicious actors are incentivized to manipulate the data stream.  We show that the CMS and HK structures can be forced to make significant estimation errors, by concrete attacks that exploit adaptivity.  We analyze these attacks analytically and experimentally, with tight agreement between the two.  Sadly, these negative results seem unavoidable for (at least) sketch-based CFEs with parameters that are reasonable in practice. On the positive side, we give a new CFE (Count-Keeper) that can be seen as a composition of the CMS and HK structures. Count-Keeper estimates are typically more accurate (by at least a factor of two) than CMS for ``honest" streams; our attacks against CMS and HK are less effective (and more resource intensive) when used against Count-Keeper; and Count-Keeper has a native ability to flag estimates that are suspicious, which neither CMS or HK (or any other CFE, to our knowledge) admits.

The use of probabilistic data structures (PDS) has grown rapidly in recent years in correlation with the rise of distributed applications producing and processing huge amounts of data. Probabilistic data structures provide compact representations of (potentially massive) data, and support a small set of queries.  The trade-off for compactness is that query responses are only guaranteed to be ``close" to the true answer (i.e., if the query were evaluated on the full data) with a certain probability. For example, the ubiquitous Bloom filter~\cite{bloom1970space} admits data-membership queries (\emph{Does element $x$ appear in the data?}).  Bloom filters are used in applications such as increasing cache performance~\cite{maggs2015}, augmenting the performance of database queries~\cite{dean2006}, indexing search results~\cite{goodwin2017bitfunnel}, and Bitcoin wallet synchronization~\cite{bip-0037}.
The probabilistic guarantee on the correctness of responses assumes that the data represented by the Bloom filter is independent of the randomness used to sample the hash functions that are used to populate the filter, and to compute query responses.  This is equivalent to providing correctness guarantees in the presence of adversarial data sets and queries that are \emph{non-adaptive}, i.e., made in advance of the sampling of the hash functions.  
A number of recent works ---~notably those of Naor and Yogev~\cite{naor2015bloom}, Clayton, Patton and Shrimpton~\cite{clayton2019}, and Fili\'{c} et al.~\cite{_CCS:FPUV22}~--- have provided detailed analyses of Bloom filters under \emph{adaptive} attacks; the results are overwhelmingly negative. Paterson and Raynal~\cite{paterson2021} provided similar results for the HyperLogLog PDS, which can be used to count the number of distinct elements in a data collection~\cite{flajolet2007hyperloglog}.

In this work, we focus on PDS that can be used to estimate the number of times any particular element~$x$ appears in a collection of data, i.e., the \emph{frequency} of~$x$. Such compact frequency estimators (CFEs) are commonly used in streaming settings, to identify elements with the largest frequencies ---~so-called \emph{heavy hitters} or \emph{elephants}.  Finding extreme elements is important for network planning~\cite{feldman00}, network monitoring~\cite{lakhina04}, recommendation systems~\cite{melis2015efficient}, and approximate database queries~\cite{redisbloom}, to name a few applications.  

The Count-min Sketch (CMS)~\cite{cormode2005improved} and HeavyKeeper (HK)~\cite{yang2019heavykeeper} structures are two CFEs that we consider, in detail.
The CMS structure has been widely applied to a number of problems outlined above. Details on these applications are thoroughly examined in the survey paper by Sigurleifsson et al.~\cite{sigurleifsson2019overview}. 
The HK structure is the CFE of choice in the RedisBloom module~\cite{redisbloom}, a component of the Redis database system~\cite{redis}.  

Of particular interest to us is the 2019 ACM SIGSAC work of Clayton, Patton, and Shrimpton~\cite{clayton2019} that both furthers the adversarial analysis on Bloom filters and also presents a general model for analyzing probabilistic data structures for provable security. This paper gives a first look at the security of the Count-min sketch in adversarial environments. However, in this paper a very conservative security model for the CMS was used, which counted any overestimation of a particular element as an adversarial gain, rather than tying the security to the non-adaptive guarantees of the structure. Further, a thresholding mechanism is used to achieve security for the CMS, a solution which we deem untenable for real world uses of the CMS. 

As is the case for Bloom filters, HyperLogLog and other PDS, the accuracy guarantees for CFEs effectively assume that the data they represent were produced by a non-adaptive strategy.  Our work explores the accuracy of CMS and HK estimates when the data is produced by \emph{adaptive} adversarial strategies (i.e., adaptive attacks).  We give explicit attacks that aim to make as-large-as-possible gaps between the estimated and true frequencies of data elements.  We give concrete, not asymptotic, expressions for these gaps, in terms of specific adversarial resources (i.e., oracle queries), and support these expressions with experimental results.  And our attacks fit within a well-defined ``provable security"-style attack model that captures four adversarial access settings: whether the CFE representations are publicly exposed (at all times) or hidden from the adversary, and whether the internal hash functions are public (i.e., computable offline) or private (i.e. visible only, if at all, by online interaction with the structure).

In this work we draw explicit attention to the fact that probabilistic data structures, and in particular frequency estimators, were not designed with security in mind by presenting attacks that degrade the correctness of the query responses these structures provide.  

Our findings are negative in all cases.  No matter the combination of public and private, a well resourced adversary can force CMS and HK estimates to be arbitrarily far from the true frequency. As one example of what this means for larger systems, things that have never appeared in the stream can be made to look like heavy hitters (in the case of CMS), and legitimate heavy hitters can be made to disappear entirely (in the case of HK).  This is somewhat surprising in the ``private-private'' setting, where the attack can only gain information about the structure and its operations via frequency estimate queries.  Of course, there are differences in practice: when attacks are forced to be online, they are easier to detect and throttle, so the query-resource terms in our analytical results are likely capped at smaller values than when some or all of an attack can progress offline. 

Our attacks exploit structural commonalities of CMS and HK.  At their core, each of these processes incoming data elements by mapping them to multiple positions in an array of counters, and these are updated according to simple, structure-specific rules. Similarly, 
when frequency estimation (or \emph{point}) queries are made, the queried element is mapped to its associated positions, and the response is computed as a simple function of values they hold.  So, our attacks concern themselves with finding \emph{cover sets}: given a target~$x$, find a small set of data elements (not including~$x$) that collectively hash to all of the positions associated with~$x$. Intuitively, inserting a cover set for~$x$ into the stream will give the structure incorrect information about~$x$'s relationship to the stream, causing it to over- or underestimate its frequency. 

The existence of a cover set in the represented data is necessary for producing frequency estimation errors in HK, and both necessary and sufficient in CMS.  Sadly, our findings suggest that preventing an adaptive adversary from finding such a set seems futile, no matter what target element is selected.  The task can be made harder by increasing the structural parameters, but this quickly leads to structures whose size makes them unattractive in practice, i.e., \emph{linear} in the length of the stream.

\paragraph{Motivating a more robust CFE} Say that the array~$M$ in CMS has~$k$ rows and $m$~counters (columns) per row.
The CMS estimate for~$x$ is $\hat{n}_x=\min_{i \in [k]} \{M[i][p_i]\}$, where $p_i$ is the position in row~$i$ to which~$x$ hashes.  In the insertion-only stream model it must be that $\hat{n}_x \geq n_x$, where~$n_x$ is the true frequency of~$x$. To see this, given an input stream~$\streamvar{S}$,
let $V^{i}_{x}=\{y \in \streamvar{S} \,|\, y\neq x \mbox{ and } h_i(y)=p_i\}$ be the set of elements that hash to the same counter as~$x$, in the $i$-th row.  Then we can write $M[i][p_i]=n_x + \sum_{y \in V^i_x }n_y$, where the $n_y > 0$ are the true frequencies of the colliding~$y$s.
Viewed this way, we see that the CMS estimate~$\hat{n}_x$ minimizes the impact of ``collision noise'', i.e., 
$\hat{n}_x = n_x + \min_{i \in [k]}\{\sum_{y \in V^i_x }n_y\}$.  


We could improve this estimate if we knew some extra information about the value of the sum, or the elements that contribute to it. 
Let's say that, with a reasonable amount of extra space, we could compute $C_i = \epsilon_i \left(\sum_{y \in V^i_x }n_y\right)$ for some $\epsilon_i \in [0,1]$ that is bounded away from zero. Then we would improve the estimate to
$
\hat{n}_x = n_x + \min_{i \in [k]}\left\{(1-\epsilon_i)\left(\sum_{y \in V^i_x }n_y\right)\right\}
$.
How might we do this? Consider the case that for some row~$i \in [k]$ there is an element~$y^* \in V^{i}_{x}$ that dominates the collision noise, e.g. $n_{y^*} = (1/2)\sum_{y \in V^i_x }n_y$.  Then even the ability to accurately estimate $n_{y^*}$ would give a significant improvement in accuracy of $\hat{n}_x$, by setting~$C_i$ to this estimate. It turns out that HK provides something like this. It maintains a $k \times m$ matrix~$A$, where $A[i][j]$ holds a pair $(\mathrm{fp},\mathrm{cnt})$. In the first position is a \emph{fingerprint} of the current ``owner'' of this position, and, informally, $\cnt$ is the number of times that~$A[i][j]$ ``remembers'' seeing the current owner.  (Ownership can change over time, as we describe in the body.) If we use the same hash functions to map element~$x$ into the same-sized~$M$ and~$A$, then there is possibility of using the information at~$A[i][p_i]$ to reduce the additive error (w.r.t. $n_x$) in the value of~$M[i][p_i]$.  This observation forms the kernel of our new Count-Keeper structure.


\paragraph{The Count-Keeper CFE}

We propose a new structure that, roughly speaking, combines equally sized (still compact) CMS and HK structures, and provide analytical and empirical evidence that it reduces the error (by at least a factor of two) that can be induced once a cover set is found. It also requires a type of cover set that is roughly twice as expensive (in terms of oracle queries) to find. Moreover, it can effectively detect when the reported frequency of an element is likely to have large error. In this way we can dampen the effect of the attacks, by catching and raising a \emph{flag} when a cover set has been found and is inserted many times to induce a large frequency error estimation on a particular element.

Intuitively, our Count-Keeper (CK) structure has improved robustness against adaptive attacks because CMS can only overestimate the frequency of an element, and HK can only underestimate the frequency (under a certain, practically reasonable assumption). We experimentally demonstrate that CK is robust against a number of attacks we give against the other structures. Moreover, it performs comparably well if not better than the other structures we consider in frequency estimation tasks in the non-adversarial setting.

As a side note, we uncovered numerous analytical errors in~\cite{yang2019heavykeeper} that invalidate some of their claims about the behaviors of the HK structure.  We have communicated with the authors of~\cite{yang2019heavykeeper} and contacted Redis, whose RedisBloom library implements HK (and CMS) with fixed, public hash functions (i.e., the internal randomness is fixed for all time and visible to attackers).

In \cite{HassidimKMMS20}, the authors consider adding robustness to streaming algorithms using differential privacy.
Meanwhile, Hardt and Woodruff~\cite{hardt2013}, Cohen et al.~\cite{cohen2022robust} and Ben-Eliezer et al.\cite{BenEliezer2022} have shown that linear sketches (including CMS but not HK) are not ``robust" to well-resourced adaptive attacks, when it comes to various $L_p$-norm estimation tasks, e.g., solving the $k$-heavy-hitters problem relative to the $L_2$-norm.  
These works are mostly of theoretical importance, whereas we aim to give concrete  attacks and results that are (more) approachable for practitioners.
