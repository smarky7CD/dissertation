\chapter{Introduction}

\authorRemark{The following needs to be re-organized and re-written.}

Data structures define representations of possibly dynamic (multi)sets, along with the operations that can be performed on this representation of the underlying data. Efficient data structures are crucial for designing efficient algorithms~\cite{clrs}. The development and analysis of data structures has largely been driven by operational concerns, e.g., efficiency, ease of deployment, support for broad application. Security concerns, on the other hand, have traditionally been afterthoughts (at best). However, recent research has highlighted that many widely-used data structures do not behave as expected when in the presence of adversaries that have the ability to control the data they represent. Further, complex protocols that have sophisticated security goals are increasingly using a variety of bespoke data structures as fundamental components of their design. Therefore, it is wise to begin applying the provable security paradigm to data structures themselves. 

For instance, consider \emph{compressing probabilistic data structures} (CPDS). The use of CPDS has grown rapidly in recent years in correlation with the rise of distributed applications producing and processing colossal amounts of data. Probabilistic data structures provide compact representations of (potentially massive) data, and support a small set of queries.  The trade-off for compactness is that query responses are only guaranteed to be ``close" to the true answer (i.e., if the query were evaluated on the full data) with a certain probability. For example, the ubiquitous Bloom filter~\cite{bloom1970space} admits data-membership queries (\emph{Does element $x$ appear in the data?}).  Bloom filters are used in applications such as increasing cache performance~\cite{maggs2015}, augmenting the performance of database queries~\cite{dean2006}, indexing search results~\cite{goodwin2017bitfunnel}, and Bitcoin wallet synchronization~\cite{bip-0037}.
The probabilistic guarantee on the correctness of responses assumes that the data represented by the Bloom filter is independent of the randomness used to sample the hash functions that are used to populate the filter, and to compute query responses.  This is equivalent to providing correctness guarantees in the presence of adversarial data sets and queries that are \emph{non-adaptive}, i.e., made in advance of the sampling of the hash functions.  A number of recent works ---~notably those of Naor and Yogev~\cite{naor2015bloom}, Clayton, Patton and Shrimpton~\cite{clayton2019}, and Fili\'{c} et al.~\cite{_CCS:FPUV22}~--- have provided detailed analyses of Bloom filters under \emph{adaptive} attacks; the results are overwhelmingly negative. Paterson and Raynal~\cite{paterson2021} provided similar results for the HyperLogLog PDS, which can be used to count the number of distinct elements in a data collection~\cite{flajolet2007hyperloglog}.



Moreover, consider the class of data structures we refer to as \emph{probabilistic skipping-based data structures} (PSDS). Unlike the probabilistic data structures we discussed earlier, this class of structure are not space-efficient (compact) and, in turn, give exact answers to queries. These data structures (e.g., hash tables, skip lists, and treaps) offer fast average-case runtime of their operations, but have worst-case runtime that is linear in the size of the collection they represent. They achieve this fast average-case performance by using some form of randomness to determine the representation of the underlying data collection.  Recent research shows that adaptive adversaries are able to force worst-case runtime for these structure, often demonstrated by attacks on real-world systems. Therefore, instead of focusing on adversarial correctness as with CPDS, we focus on preserving the expected run time of these structures with large probability in the presence of an adversary.

Count-Min Sketch (CMS) and HeavyKeeper (HK) are two realizations of a compact frequency estimator (CFE).  Recall, these are a class of compressing probabilistic data structures that maintain a compact summary of (typically) high-volume streaming data, and provides approximately correct estimates of the number of times any particular element has appeared. CFEs are often the base structure in systems looking for the highest-frequency elements (i.e., top-$K$ elements, heavy hitters, elephant flows).  Traditionally, probabilistic guarantees on the accuracy of frequency estimates are proved under the implicit assumption that stream elements do not depend upon the internal randomness of the structure. Said another way, they are proved in the presence of data streams that are created by non-adaptive adversaries.  Yet in many practical use-cases, this assumption is not well-matched with reality; especially, in applications where malicious actors are incentivized to manipulate the data stream.  We show that the CMS and HK structures can be forced to make significant estimation errors, by concrete attacks that exploit adaptivity.  We analyze these attacks analytically and experimentally, with tight agreement between the two.  Sadly, these negative results seem unavoidable for (at least) sketch-based CFEs with parameters that are reasonable in practice. On the positive side, we give a new CFE (Count-Keeper) that can be seen as a composition of the CMS and HK structures. Count-Keeper estimates are typically more accurate (by at least a factor of two) than CMS for ``honest" streams; our attacks against CMS and HK are less effective (and more resource intensive) when used against Count-Keeper; and Count-Keeper has a native ability to flag estimates that are suspicious, which neither CMS or HK (or any other CFE, to our knowledge) admits.


\section{Outline}

\section{Thesis Statement}